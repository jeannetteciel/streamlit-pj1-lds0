{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMPuaZBIFvNq2lrwWpi84N7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ckdur9JWXAJC","executionInfo":{"status":"ok","timestamp":1733554237097,"user_tz":-420,"elapsed":4592,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}},"outputId":"854b1cc8-c786-4034-fbaa-facd9cf30263"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.40.2)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n","Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"]}],"source":["!pip3 install streamlit"]},{"cell_type":"code","source":["!pip3 install pyngrok"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63CdEnMlXQT-","executionInfo":{"status":"ok","timestamp":1733554239922,"user_tz":-420,"elapsed":2828,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}},"outputId":"334328d5-9022-400c-872b-27dbc5a36831"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"]}]},{"cell_type":"code","source":["!pip install underthesea"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qa-UFKKQLk39","executionInfo":{"status":"ok","timestamp":1733559243932,"user_tz":-420,"elapsed":5286,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}},"outputId":"96027b6d-f098-426e-c829-489680767c80"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting underthesea\n","  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n","Collecting python-crfsuite>=0.9.6 (from underthesea)\n","  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.9.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.32.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.4.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.5.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.2)\n","Collecting underthesea-core==1.0.4 (from underthesea)\n","  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2024.9.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.8.30)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.13.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.5.0)\n","Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\n","Successfully installed python-crfsuite-0.9.11 underthesea-6.8.4 underthesea-core-1.0.4\n"]}]},{"cell_type":"code","source":["from google.colab import drive"],"metadata":{"id":"Qk65lLfNmyrQ","executionInfo":{"status":"ok","timestamp":1733554239922,"user_tz":-420,"elapsed":4,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJlDY0Ftm58C","executionInfo":{"status":"ok","timestamp":1733554242055,"user_tz":-420,"elapsed":2136,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}},"outputId":"081906c6-d45c-435b-fea6-870ce56315d2"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","import pandas as pd\n","import numpy as np\n","import regex\n","import regex\n","import string\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud, STOPWORDS\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, classification_report\n","import joblib\n","from underthesea import sent_tokenize, word_tokenize\n","from nltk.probability import FreqDist\n","\n","df = pd.read_csv('/content/drive/MyDrive/DL07_K299_HaThuyAn_TruongThanhTuyen/DL07_K299_TruongThanhTuyen/Project 1/data_for_model.csv')\n","\n","### For New Prediction\n","# Chuyển nội dung về chữ thường trước khi remove duplicate\n","df.processed_cmt = df.processed_cmt.str.lower()\n","\n","df['processed_cmt'] = df['processed_cmt'].fillna('')\n","X_train, X_test, y_train, y_test = train_test_split(df['processed_cmt'], df['rating_group'], test_size=0.3, random_state=1)\n","vectorize = TfidfVectorizer()\n","X_train_tfidf = vectorize.fit_transform(X_train)\n","X_test_tfidf = vectorize.transform(X_test)\n","rus = RandomUnderSampler(random_state=0)\n","X_train_resampled, y_train_resampled = rus.fit_resample(X_train_tfidf, y_train)\n","model = LogisticRegression()\n","model.fit(X_train_resampled, y_train_resampled)\n","result = model.predict(X_test_tfidf)\n","\n","train_acc = model.score(X_train_resampled, y_train_resampled)\n","test_acc = model.score(X_test_tfidf, y_test)\n","\n","rp =classification_report(y_test, result)\n","\n","joblib.dump(model, '/content/drive/MyDrive/DL07_K299_HaThuyAn_TruongThanhTuyen/DL07_K299_TruongThanhTuyen/Project 1/logisticregression.joblib')\n","loaded_model = joblib.load('/content/drive/MyDrive/DL07_K299_HaThuyAn_TruongThanhTuyen/DL07_K299_TruongThanhTuyen/GUI/logisticregression.joblib')\n","\n","###\n","\n","### For Product Analysis\n","# Lấy 20 sản phẩm\n","random_products = df.head(n=1000)\n","# print(random_products)\n","\n","st.session_state.random_products = random_products\n","\n","# Kiểm tra xem 'selected_ma_san_pham' đã có trong session_state hay chưa\n","if 'selected_ma_san_pham' not in st.session_state:\n","    # Nếu chưa có, thiết lập giá trị mặc định là None hoặc ID sản phẩm đầu tiên\n","    st.session_state.selected_ma_san_pham = None\n","\n","# Tạo set các stopwords:\n","stopwords = set()\n","f = open(r'/content/drive/MyDrive/LDS0_K292_Online_TruongThanhTuyen/Project 1/files/vietnamese-stopwords.txt', \"r\", encoding='utf-8')\n","for line in f:\n","    word = f.readline()\n","    stopwords.add(word.replace('\\n',''))\n","f.close()\n","\n","list_of_words = ['và', 'một', 'của', 'có', 'đó', 'rất', 'nào', 'được',\n","                'khi', 'thể', 'sự', 'tính', 'trong','cũng','cùng','cho','hay','chỉ']\n","for word in list_of_words:\n","    stopwords.add(word)\n","\n","pos = pd.read_csv('/content/drive/MyDrive/DL07_K299_HaThuyAn_TruongThanhTuyen/DL07_K299_TruongThanhTuyen/Project 1/positive_words.csv')\n","positive_words = pos['positive_words'].to_list()\n","positive_words_u = []\n","for i in positive_words:\n","  a = i.replace(\" \", \"_\")\n","  positive_words_u = positive_words_u + [a]\n","\n","neg = pd.read_csv('/content/drive/MyDrive/DL07_K299_HaThuyAn_TruongThanhTuyen/DL07_K299_TruongThanhTuyen/Project 1/negative_words.csv')\n","negative_words = neg['negative_words'].to_list()\n","negative_words_u = []\n","for i in negative_words:\n","  a = i.replace(\" \", \"_\")\n","  negative_words_u = negative_words_u + [a]\n","\n","def process_comment(comment):\n","    # Tokenize các từ\n","    tokens = word_tokenize(comment)\n","    # Loại bỏ stop words và các kí tự không cần thiết\n","    stop_words = stopwords\n","    filtered_tokens = [word for word in tokens if word not in stop_words]\n","    return filtered_tokens\n","\n","# Xử lý các từ trong comment và tính toán các từ thường gặp\n","def cmt_extract (df, comment_column):\n","  all_words = []\n","  positive_list = []\n","  negative_list = []\n","  for index, row in df.iterrows():\n","      comment = row[comment_column]\n","      tokens = process_comment(comment)\n","      all_words.extend(tokens)\n","      if row['rating_group'] == 'positive' and any(token in positive_words for token in tokens):\n","          positive_list.extend(tokens)\n","      elif row['rating_group'] == 'negative' and any(token in negative_words for token in tokens):\n","          negative_list.extend(tokens)\n","  return all_words, positive_list, negative_list\n","\n","\n","# App Design\n","st.title('GUI Đề án tốt nghiệp DS')\n","st.subheader('Project 1 - Sentiment Analysis')\n","\n","menu = ['New Prediction', 'Product Analysis']\n","choice = st.sidebar.selectbox('Menu', menu)\n","st.sidebar.write(\"\"\"#### Thành viên thực hiện:\n","                 Hà Thúy An & Trương Thanh Tuyền\"\"\")\n","st.sidebar.write(\"\"\"#### Giảng viên hướng dẫn:\n","                (Cô) Khuất Thùy Phương\"\"\")\n","st.sidebar.write(\"\"\"#### Thời gian thực hiện: 12/2024\"\"\")\n","\n","if choice == 'New Prediction':\n","  st.subheader('Apply Logistic Regression Model to predict a new comment negative or positive')\n","  st.write(\" Input or Load new comments\")\n","  flag = False\n","  lines = None\n","  type = st.radio(\"Upload data or Input data?\", options=(\"Upload\", \"Input\"))\n","  if type == 'Upload':\n","    uploaded_file = st.file_uploader(\"Choose a file\", type = ['csv', 'txt'])\n","    if uploaded_file is not None:\n","            lines = pd.read_csv(uploaded_file, header=None)\n","            st.dataframe(lines)\n","            lines = lines[0]\n","            flag = True\n","  if type==\"Input\":\n","      content = st.text_area(label=\"Input your content:\")\n","      if content!=\"\":\n","          lines = content.split('\\n')  # Split the input by newline characters\n","          lines = [line.strip() for line in lines if line.strip()] # Remove empty lines\n","          flag = True\n","\n","  if flag:\n","    st.write(\"Content:\")\n","    if len(lines)>0:\n","        st.code(lines)\n","\n","        # Convert lines to a list if it's a NumPy array or Series\n","        if isinstance(lines, (np.ndarray, pd.Series)):\n","            lines = lines.tolist()\n","\n","        # Vectorize and predict for each line\n","        new_comment_vectorized = vectorize.transform(lines)\n","        prediction = loaded_model.predict(new_comment_vectorized)\n","\n","        # Create a DataFrame for the table\n","        data = {'Comment': lines, 'Prediction': prediction}\n","        df_predictions = pd.DataFrame(data)\n","\n","        # Display the table using st.dataframe\n","        st.dataframe(df_predictions)\n","\n","elif choice == 'Product Analysis':\n","  st.subheader('Get to know your product')\n","  # Theo cách cho người dùng chọn sản phẩm từ dropdown\n","  # Get unique product names and codes\n","  unique_products = st.session_state.random_products[['ten_san_pham', 'ma_san_pham']].drop_duplicates(subset=['ten_san_pham'])\n","  # Tạo một tuple cho mỗi sản phẩm, trong đó phần tử đầu là tên và phần tử thứ hai là ID\n","  product_options = [(row['ten_san_pham'], row['ma_san_pham']) for index, row in unique_products.iterrows()]\n","  st.session_state.random_products\n","  # Tạo một dropdown với options là các tuple này\n","  selected_product = st.selectbox(\n","    \"Chọn sản phẩm\",\n","    options=product_options,\n","    format_func=lambda x: x[0]  # Hiển thị tên sản phẩm\n","  )\n","  # Display the selected product\n","  st.write(\"Bạn đã chọn:\", selected_product)\n","  # Cập nhật session_state dựa trên lựa chọn hiện tại\n","  st.session_state.selected_ma_san_pham = selected_product[1]\n","  product_code = st.session_state.selected_ma_san_pham\n","  # Lọc dữ liệu cho sản phẩm có mã tương ứng\n","  product_data = df[df['ma_san_pham'] == product_code]\n","\n","  all_words, positive_list, negative_list = cmt_extract(product_data, 'processed_cmt')\n","\n","  # Số lượt đánh giá\n","  num_reviews = len(product_data)\n","\n","  # Điểm trung bình\n","  average_rating = product_data['so_sao'].median()\n","\n","  # Số lượng đánh giá của mỗi loại (tích cực - tiêu cực - trung bình)\n","  positive_reviews = len(product_data[product_data['rating_group'] == 'positive'])\n","  negative_reviews = len(product_data[product_data['rating_group'] == 'negative'])\n","\n","  # Các từ thường gặp trong comment\n","  freq_dist = FreqDist(all_words)\n","  freq_dist_positive = FreqDist(positive_list)\n","  freq_dist_negative = FreqDist(negative_list)\n","\n","  # Các từ thường xuất hiện phần đánh giá tích cực và tiêu cực\n","  common_positive_words = freq_dist_positive.most_common(10)\n","  common_negative_words = freq_dist_negative.most_common(10)\n","\n","  positive_comment_words =' '.join(positive_list)\n","  negative_comment_words =' '.join(negative_list)\n","\n","  cmt = product_data[['noi_dung_binh_luan', 'rating_group']]\n","  if st.session_state.selected_ma_san_pham:\n","    st.write(\"ma_san_pham: \", st.session_state.selected_ma_san_pham)\n","    # Hiển thị thông tin sản phẩm được chọn\n","    selected_product = df[df['ma_san_pham'] == st.session_state.selected_ma_san_pham]\n","\n","    if not selected_product.empty:\n","      st.write('## Đánh giá sản phẩm')\n","      st.write('### ', selected_product['ten_san_pham'].values[0])\n","      st.write('Số lượt đánh giá:', num_reviews)\n","      st.write('Điểm trung bình:', average_rating)\n","      st.write('Số lượng đánh giá tích cực:', positive_reviews)\n","      st.write('Số lượng đánh giá tiêu cực:', negative_reviews)\n","\n","      st.write('#### Từ thường xuất hiện ở đánh giá tích cực: ')\n","      # Vẽ wordclouds\n","      wc_like=WordCloud(background_color='white', max_words=1000, stopwords=stopwords)\n","      wc_like.generate(positive_comment_words)\n","      plt.figure(figsize=(10, 12))\n","      plt.imshow(wc_like, interpolation='bilinear')\n","      plt.axis('off')\n","      plt.show()\n","      st.pyplot(plt)\n","\n","      st.write('#### Từ thường xuất hiện ở đánh giá tiêu cực: ')\n","      # Vẽ wordclouds\n","      wc_like=WordCloud(background_color='white', max_words=1000, stopwords=stopwords)\n","      wc_like.generate(negative_comment_words)\n","      plt.figure(figsize=(10, 12))\n","      plt.imshow(wc_like, interpolation='bilinear')\n","      plt.axis('off')\n","      plt.show()\n","      st.pyplot(plt)\n","    else:\n","      st.write(f\"Không tìm thấy sản phẩm với ID: {st.session_state.selected_ma_san_pham}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGa3QFYmXaIX","executionInfo":{"status":"ok","timestamp":1733560649900,"user_tz":-420,"elapsed":317,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}},"outputId":"630c9bd0-5de6-4e98-c7b5-d1427f71424a"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok"],"metadata":{"id":"6b9XHtzUX_sa","executionInfo":{"status":"ok","timestamp":1733554270730,"user_tz":-420,"elapsed":9,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["ngrok.set_auth_token('2psCD4VnJm8nJhtP8JlNYTSXz6r_4PQRHWsfbfpt7BNpXeiv8')"],"metadata":{"id":"B_UHlKk6YB38","executionInfo":{"status":"ok","timestamp":1733554270730,"user_tz":-420,"elapsed":9,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["# Start Streamlit server on a specific port\n","!nohup streamlit run app.py --server.port 8501 &\n","\n","# Start ngrok tunnel to expose the Streamlit server\n","ngrok_tunnel = ngrok.connect(addr='8501', proto='http', bind_tls=True)\n","\n","# print the URL of the ngrok tunnel\n","print(' * Tunnel URL:', ngrok_tunnel.public_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bmj842CbYFnR","executionInfo":{"status":"ok","timestamp":1733554270730,"user_tz":-420,"elapsed":8,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}},"outputId":"7a37f1dd-e01f-41fa-e9c5-128b7834c093"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["nohup: appending output to 'nohup.out'\n"," * Tunnel URL: https://5383-34-106-41-116.ngrok-free.app\n"]}]},{"cell_type":"code","source":["# ngrok.kill()"],"metadata":{"id":"gW79EnNmmuFu","executionInfo":{"status":"ok","timestamp":1733554270730,"user_tz":-420,"elapsed":5,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download('D:/Data - Analytics/CSC_DataScience_MachineLearning/LDS0/GUI/streamlit-project1.py')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"n_xacvRwTBML","executionInfo":{"status":"error","timestamp":1733561275877,"user_tz":-420,"elapsed":293,"user":{"displayName":"Trương Thanh Tuyền","userId":"13037839301500990757"}},"outputId":"88151af0-216b-4923-d718-80839531e5b0"},"execution_count":117,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"Cannot find file: D:/Data - Analytics/CSC_DataScience_MachineLearning/LDS0/GUI/streamlit-project1.py","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-117-72b3d3311189>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'D:/Data - Analytics/CSC_DataScience_MachineLearning/LDS0/GUI/streamlit-project1.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: D:/Data - Analytics/CSC_DataScience_MachineLearning/LDS0/GUI/streamlit-project1.py"]}]}]}